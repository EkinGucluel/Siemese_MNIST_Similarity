{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45808d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Subset, random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        # Store the data\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Extract all (image, label) pairs\n",
    "        data = [(img, label) for img, label in base_dataset]\n",
    "\n",
    "        # Create all combinations\n",
    "        for (img1, label1), (img2, label2) in product(data, data):\n",
    "            self.pairs.append((img1, img2))\n",
    "            self.labels.append(int(label1 == label2))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return ((image_A, image_B), label) for this index\n",
    "        img1, img2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return (img1, img2), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0505af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module is the parent class\n",
    "        # NOT calling these functions here, just storing the layer\n",
    "        # Kalıp Gibi.\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1) # 1 for grayscale img, 64 feature maps, 3x3 convolution window, 1 pixel move for each step, 1-pixel border around the image, gives output of shape (batch_size, 64, 28, 28)\n",
    "        self.bn1 = nn.BatchNorm2d(64) # Normalization, keeps its mean 0 and variance 1\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # (in_channels, out_channels, kernel_size, ...)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1) # (batch_size, 512, 28, 28) --> (batch_size, 512). squeezes each 28 × 28 feature map into a single number, using average pooling.\n",
    "\n",
    "        self.embedding = nn.Linear(512, 64) # Like Dense(64) in Keras, \"Take this 512-dimensional vector (summary of the image), and map it down to 64 numbers.\"\n",
    "\n",
    "        # After CNN:\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 1) # final output layer\n",
    "\n",
    "    def cnn(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # runs the convolution layer on input x, normalizes the result, applies ReLU activation\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        x = self.global_pool(x) # shape: [batch, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1) # shape: [batch, 512]\n",
    "        x = self.embedding(x) # shape: [batch, 64]\n",
    "        return x\n",
    "\n",
    "    def forward(self, imgA, imgB):\n",
    "        # Extract features from both images\n",
    "        featA = self.cnn(imgA)               # shape: [batch, 64]\n",
    "        featB = self.cnn(imgB)\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([featA, featB], dim=1) # shape: [batch, 128]\n",
    "\n",
    "        # Compare / MLP (Multi-Layer Perceptron)\n",
    "        x = F.relu(self.fc1(x))              # shape: [batch, 64]\n",
    "        x = torch.sigmoid(self.out(x))       # shape: [batch, 1] — similarity score, sigmoid activation function, output between [0-1]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2f69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datayı hazırlamak için 1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                            # Converts to tensor and scales to [0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))        # Normalizes using MNIST mean and std\n",
    "])\n",
    "# loading the full dataset\n",
    "full_train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459d0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 200 # Datayı hazırlamak için 2\n",
    "validation_size = 50\n",
    "test_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85aa608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datayı hazırlamak için 3\n",
    "train_validation_subset, _ = random_split(full_train_dataset, [train_size + validation_size, len(full_train_dataset) - (train_size + validation_size)], generator=torch.Generator().manual_seed(42))\n",
    "train_subset, validation_subset = random_split(train_validation_subset, [train_size, validation_size], generator=torch.Generator().manual_seed(42))\n",
    "test_subset, _ = random_split(test_dataset, [test_size, len(test_dataset) - test_size], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d12f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "# Datayı hazırlamak için 4\n",
    "paired_train_dataset = PairedDataset(train_subset)\n",
    "paired_val_dataset = PairedDataset(validation_subset)\n",
    "paired_test_dataset = PairedDataset(test_subset)\n",
    "print(len(paired_train_dataset))  # 35×35 = 1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0f3fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datayı hazırlamak için 5\n",
    "batch_size = 8\n",
    "\n",
    "# Batches the data\n",
    "train_loader = DataLoader(paired_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(paired_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(paired_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9b3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n",
      "torch.Size([8, 1, 28, 28])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for (imgA, imgB), labels in train_loader:\n",
    "    print(imgA.shape)\n",
    "    print(imgB.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c217d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de2db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss, used to compute the loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam optimizer updates model weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da02657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Loss: 0.3116\n",
      "Validation Accuracy: 0.8920\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [2/20] - Loss: 0.2497\n",
      "Validation Accuracy: 0.8976\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [3/20] - Loss: 0.1744\n",
      "Validation Accuracy: 0.9284\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [4/20] - Loss: 0.0883\n",
      "Validation Accuracy: 0.9420\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [5/20] - Loss: 0.0379\n",
      "Validation Accuracy: 0.9292\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [6/20] - Loss: 0.0259\n",
      "Validation Accuracy: 0.9404\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [7/20] - Loss: 0.0138\n",
      "Validation Accuracy: 0.9508\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [8/20] - Loss: 0.0179\n",
      "Validation Accuracy: 0.9412\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [9/20] - Loss: 0.0139\n",
      "Validation Accuracy: 0.9228\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [10/20] - Loss: 0.0089\n",
      "Validation Accuracy: 0.8920\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [11/20] - Loss: 0.0104\n",
      "Validation Accuracy: 0.9476\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [12/20] - Loss: 0.0160\n",
      "Validation Accuracy: 0.9504\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [13/20] - Loss: 0.0057\n",
      "Validation Accuracy: 0.9536\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [14/20] - Loss: 0.0091\n",
      "Validation Accuracy: 0.9508\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [15/20] - Loss: 0.0091\n",
      "Validation Accuracy: 0.9564\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [16/20] - Loss: 0.0051\n",
      "Validation Accuracy: 0.9460\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [17/20] - Loss: 0.0034\n",
      "Validation Accuracy: 0.9460\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [18/20] - Loss: 0.0076\n",
      "Validation Accuracy: 0.9496\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [19/20] - Loss: 0.0109\n",
      "Validation Accuracy: 0.9464\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n",
      "Epoch [20/20] - Loss: 0.0039\n",
      "Validation Accuracy: 0.9512\n",
      "Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (imgA, imgB), labels in train_loader:\n",
    "        imgA, imgB, labels = imgA.to(device), imgB.to(device), labels.to(device) # Moves data to GPU\n",
    "        outputs = model(imgA, imgB)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1)) # compares models prediction and the correct answer\n",
    "        optimizer.zero_grad() # Clear previous gradients (weight change needs)\n",
    "        loss.backward() # Figures out how much each weight needs to change to make the prediction better\n",
    "        optimizer.step() # Changing the weights using the advice calculated by .backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient tracking for faster performance\n",
    "        for (imgA, imgB), labels in validation_loader:\n",
    "            imgA, imgB, labels = imgA.to(device), imgB.to(device), labels.to(device)\n",
    "            outputs = model(imgA, imgB)\n",
    "            predictions = (outputs > 0.5).float()  # Binary prediction\n",
    "            correct += (predictions == labels.unsqueeze(1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"siamese_model_epoch20_bs8_train200_testval50_20250723.pth\")\n",
    "    print(\"Model saved as siamese_model_epoch20_bs8_train200_testval50_20250723.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26925252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained model.\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved model (optional)\n",
    "model.load_state_dict(torch.load(\"siamese_model_epoch20_bs8_train200_testval50_20250723.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"Loaded pre-trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9edb0750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9160\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set to eval mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients -- INFERENCE\n",
    "    for (imgA, imgB), labels in test_loader:\n",
    "        imgA, imgB, labels = imgA.to(device), imgB.to(device), labels.to(device)\n",
    "        outputs = model(imgA, imgB)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        correct += (predictions == labels.unsqueeze(1)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d801b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (similarity score): 0.9985173345\n",
      "Actual: 1\n"
     ]
    }
   ],
   "source": [
    "idx1, idx2 = 2, 20\n",
    "img1, label1 = full_train_dataset[idx1]\n",
    "img2, label2 = full_train_dataset[idx2]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(img1.squeeze(), cmap=\"gray\")\n",
    "ax1.set_title(f\"Label A: {label1}\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(img2.squeeze(), cmap=\"gray\")\n",
    "ax2.set_title(f\"Label B: {label2}\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "img1 = img1.unsqueeze(0).to(device)\n",
    "img2 = img2.unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    similarity_score = model(img1, img2).item()\n",
    "\n",
    "print(f\"Prediction (similarity score): {similarity_score:.10f}\")\n",
    "print(f\"Actual: {int(label1 == label2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Lab 3)",
   "language": "python",
   "name": "ailab3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
